# TP — InfluxDB ↔ Telegraf ↔ Kafka - Réponses finales

## Partie 1 : Prise en main

**1. Commande de démarrage :**
```bash
docker compose up
```

**2. Observation des logs :**
On constate le lancement des services Docker un par un. Telegraf envoie des données vers outputs.file et outputs.kafka.

**3. Description des services durables :**
- **influxdb** : Base de données time-series (stocke les données météo). Reste actif pour les écritures et lectures permanentes.
- **kafka1, kafka2, kafka3** : Les trois brokers Kafka forment un cluster distribué. Ils gèrent la réception, la réplication et la distribution des messages entre producteurs et consommateurs.
- **kafdrop** : Interface web permettant de visualiser les topics, messages et partitions Kafka. Doit rester accessible.
- **telegraf** : Agent de collecte de métriques qui envoie continuellement des données à Kafka et à des fichiers.
- **inject** : Service qui envoie périodiquement des données **directement dans InfluxDB** (toutes les 5 secondes selon INTERVAL_SECONDS). **N'utilise pas Kafka.**
- **producer** : Service qui lit dans InfluxDB et envoie les données vers Kafka. Fonctionne de manière continue.

**4. Services d'initialisation (éphémères) :**
- **init-topic** : Attend que Kafka soit prêt, puis crée le topic weather (4 partitions, réplication x3) et s'arrête ensuite.
- **init-topic-telegraf** : Fait la même chose pour le topic weather-telegraf, puis s'éteint.
- **init-alert** : Configure le système d'alertes InfluxDB (checks, endpoints, notification rules), puis s'arrête.

## Partie 2 : Compréhension des fonctions

**5. Fonctionnement de Telegraf :**
Il écoute sur le port 8186 les notifications HTTP envoyées par InfluxDB (via le système d'alertes configuré par init-alert). Lorsqu'il reçoit ces métriques, il les transfère à Kafka (topic `weather-telegraf`) et les affiche dans la console (stdout).

**6. Rôle du container init-alert :**
Il configure automatiquement le système d'alertes InfluxDB en créant :
- Un **check** "push_to_kafka" qui s'exécute toutes les secondes et surveille les nouvelles données température
- Un **endpoint** HTTP "telegraf_listener" pointant vers Telegraf (http://telegraf:8186)
- Une **notification rule** qui déclenche l'endpoint quand le check passe

Cela permet à InfluxDB de notifier automatiquement Telegraf à chaque nouvelle donnée.

**7. Rôle du container init-topic-1 :**
Il attend que le broker Kafka soit prêt, puis crée le topic `weather` avec les paramètres souhaités (4 partitions, facteur de réplication 3), et s'arrête ensuite.

## Partie 3 : Compréhension des ports

**8. Ports mappés vers l'hôte :**
- **influxDB** : 8086:8086 (API + interface web)
- **kafka1** : 9092:9092 (accès broker depuis l'hôte)
- **kafdrop** : 9000:9000 (interface web)
- **kafka2 et kafka3** : pas de ports mappés (accessibles uniquement dans le réseau Docker interne)

**9. Ports utilisés par kafdrop :**
- **9000** : Interface web HTTP pour visualiser le cluster Kafka

**10. Ports utilisés par kafka3 :**
- **9092** : Listener PLAINTEXT pour les clients (producteurs/consommateurs)
- **9093** : Listener CONTROLLER pour la coordination interne du cluster KRaft entre brokers

**11. Ports utilisés par influxdb :**
- **8086** : API HTTP et interface web InfluxDB

## Partie 4 : Détails

**12. Bucket InfluxDB créé :**
- **Nom** : `weather`
- **Créateur** : InfluxDB lui-même lors du premier démarrage
- **Comment** : Via le mode `DOCKER_INFLUXDB_INIT_MODE=setup` qui détecte qu'il s'agit d'une première initialisation. Les variables d'environnement `DOCKER_INFLUXDB_INIT_BUCKET=weather`, `DOCKER_INFLUXDB_INIT_ORG=myorg`, et `DOCKER_INFLUXDB_INIT_USERNAME=admin` sont utilisées pour créer automatiquement l'utilisateur administrateur, l'organisation et le bucket par défaut.

**13. Critique de sécurité :**
Le déploiement présente plusieurs failles de sécurité majeures :
- Token hardcodé en clair (`mytoken`) dans docker-compose.yml et visible dans tous les services
- Mot de passe simple et prévisible (`adminpass`)
- Pas de chiffrement TLS/SSL pour les communications
- Credentials stockés directement dans le code source et les fichiers de configuration

**Améliorations recommandées** : Utiliser Docker secrets, un gestionnaire de secrets (HashiCorp Vault, AWS Secrets Manager), des variables d'environnement chiffrées, et activer TLS pour toutes les communications.

**14. Caractéristiques du topic sur Kafdrop :**
Sur http://localhost:9000/, on peut visualiser :
- **Topics** : `weather` et `weather-telegraf`
- **Partitions** : 4 pour chaque topic
- **Replication Factor** : 3 (chaque partition a 3 copies)
- **Leaders** : distribués sur les 3 brokers
- **ISR (In-Sync Replicas)** : normalement 3 pour chaque partition

**15. Retrouver ces caractéristiques dans le déploiement Docker :**
Dans les containers `init-topic` et `init-topic-telegraf` :
```bash
kafka-topics.sh --bootstrap-server kafka1:9092 \
  --create --if-not-exists \
  --topic weather \
  --partitions 4 \
  --replication-factor 3
```

## Partie 5 : Résilience de Kafka

**16. Arrêt d'un broker et conséquences :**

Lors de l'arrêt de kafka3 (via `docker stop kafka3`), le broker s'est arrêté proprement et a quitté le cluster Kafka. Les deux autres brokers, kafka1 et kafka2, ont continué à fonctionner, mais ont détecté que kafka3 n'était plus accessible sur le port 9093, utilisé pour la communication interne.

Étant donné que le topic avait été créé avec un facteur de réplication de 3, chaque partition possédait trois copies, une sur chaque broker. Après l'arrêt de kafka3, le cluster a temporairement perdu ces copies, rendant le topic **sous-répliqué** : il reste fonctionnel mais avec une tolérance aux pannes réduite.

Dans Kafdrop, on pouvait observer seulement deux brokers actifs, kafka1 devenant le nouveau controller (leader) du cluster. Il gère la majorité des partitions (62 %), tandis que kafka2 gère le reste (38 %). Le système continue de fonctionner, mais une nouvelle panne d'un broker pourrait mettre les données en risque.

**Comment résoudre le problème :**
Pour rétablir la santé complète du cluster et la réplication des partitions, il suffit de redémarrer kafka3 via `docker start kafka3`. Lorsqu'il revient en ligne, il se synchronise automatiquement avec les autres brokers et le cluster retrouve son état normal avec toutes les répliques restaurées.

**17. Redémarrage du broker et interaction :**

Après le redémarrage de kafka3 :

- Les brokers actifs (kafka1 et kafka2) lancent des **ReplicaFetcher threads** pour envoyer à kafka3 toutes les données manquantes depuis son arrêt.
- Kafka3 commence avec 0% de partitions en ISR (In-Sync Replicas) et remonte progressivement au fur et à mesure de la synchronisation.
- Une fois toutes les partitions à jour, kafka3 rejoint l'ISR et le cluster retrouve sa réplication complète et sa tolérance aux pannes maximale.
- Dans Kafdrop, cela se traduit par kafka3 réapparaissant et affichant sa part normale des partitions (environ 33% réparti équitablement entre les 3 brokers).

**18. Différence fondamentale entre inject et producer :**

**CORRECTION IMPORTANTE** :

- **inject** : Produit directement de nouvelles données météo simulées et les envoie **uniquement vers InfluxDB**. Il génère des températures aléatoires pour Clermont-Ferrand toutes les 5 secondes. **N'utilise pas Kafka du tout.**

- **producer** : Lit les données existantes dans InfluxDB et les transfère vers Kafka (topic `weather`). Il sert de pont pour envoyer les mesures stockées dans InfluxDB vers le système de messaging Kafka.

En résumé :
- **inject** = génération de données → InfluxDB (sans Kafka)
- **producer** = lecture InfluxDB → envoi vers Kafka

## Partie 6 : Aller plus loin

**19. Voies d'amélioration :**
- Sécuriser les credentials (vault, Docker secrets, chiffrement)
- Activer TLS/SASL pour Kafka et HTTPS pour InfluxDB
- Ajouter du monitoring (Prometheus + Grafana)
- Mettre en place des backups automatiques InfluxDB
- Ajouter des consumers Kafka pour traiter les messages
- Implémenter des tests de charge et plans de disaster recovery
- Utiliser des health checks plus robustes
- Ajouter de la documentation technique

**20. Pourquoi encapsuler le code Python dans un container :**
- **Portabilité** : Le code fonctionne de manière identique partout (dev, staging, prod)
- **Isolation** : Les dépendances Python sont isolées et ne créent pas de conflits
- **Orchestration** : Facilite le déploiement et la gestion avec Docker Compose
- **Réseau** : Communication simple via le réseau Docker interne (pas besoin de configurer des IPs)
- **Reproductibilité** : L'environnement est identique pour tous les développeurs
- **Scalabilité** : Facile de déployer plusieurs instances si nécessaire

**21. Correction de l'organigramme (schema.md) :**

Le schéma initial oublie plusieurs flux importants. Voici le schéma corrigé :

```
inject.py (génère données simulées)
    ↓
    | InfluxDB Client API
    ↓
InfluxDB (stocke température)
    ↓                    ↓
    | (via alertes)     | Flux Query API
    ↓                    ↓
Telegraf ← init-alert   producer.py
    ↓                    ↓
    | Kafka Protocol    | Kafka Protocol
    ↓                    ↓
Kafka [1..3] ← init-topic + init-topic-telegraf
    (topic weather + weather-telegraf)
    ↓
    | HTTP REST
    ↓
Kafdrop (UI monitoring)
```

**Flux de données :**
1. **inject** génère des données → **InfluxDB**
2. **InfluxDB** (via système d'alertes configuré par **init-alert**) → **Telegraf** → **Kafka** (topic `weather-telegraf`)
3. **producer** lit **InfluxDB** → **Kafka** (topic `weather`)
4. **Kafdrop** visualise l'état du cluster **Kafka**
